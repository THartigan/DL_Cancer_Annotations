{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9b167a6-57e3-43f8-b0df-5ed344629b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "save_directory = \"/local/scratch/Data/TROPHY/numpy/\"\n",
    "from Sample import Sample\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset \n",
    "import torch.nn as nn\n",
    "import torch.optim as optimise\n",
    "samples = np.load(save_directory + \"samples.npy\", allow_pickle = True)\n",
    "normalised_data = np.load(save_directory + \"sigmoid_normalised_data.npy\")\n",
    "classifications = np.load(save_directory + \"classification.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "543f065f-22dd-4a5d-8350-8172259a8998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8fdb426-424c-4762-ac4b-ca12c4a69295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "1043928\n"
     ]
    }
   ],
   "source": [
    "print(len(samples))\n",
    "print(samples[77].end_index)\n",
    "cutoff_index = samples[77].end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bff7b2c0-c931-4330-957d-bc0a456a3526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "train_data = torch.tensor(normalised_data[:cutoff_index], dtype=torch.float32).cpu() \n",
    "test_data = torch.tensor(normalised_data[cutoff_index:], dtype=torch.float32).cpu()\n",
    "train_classes = torch.tensor(classifications[:cutoff_index] - 1, dtype=torch.long).cpu()\n",
    "test_classes = torch.tensor(classifications[cutoff_index:] - 1, dtype=torch.long).cpu()\n",
    "\n",
    "print(train_classes[10])\n",
    "train_dataset = TensorDataset(train_data, train_classes)\n",
    "test_dataset = TensorDataset(test_data, test_classes)\n",
    "train_loader = DataLoader(train_dataset, batch_size=9600, shuffle=True, num_workers=14, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=9600, num_workers=14, pin_memory=True)\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_size, l1_size, l2_size, l3_size, l4_size, l5_size, output_size):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, l1_size)\n",
    "        self.layer2 = nn.Linear(l1_size, l2_size)\n",
    "        self.layer3 = nn.Linear(l2_size, l3_size)\n",
    "        self.layer4 = nn.Linear(l3_size, l4_size)\n",
    "        self.layer5 = nn.Linear(l4_size, l5_size)\n",
    "        self.layer6 = nn.Linear(l5_size, output_size)\n",
    "        self.reLU = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.reLU(self.layer1(x))\n",
    "        x = self.reLU(self.layer2(x))\n",
    "        x = self.reLU(self.layer3(x))\n",
    "        x = self.reLU(self.layer4(x))\n",
    "        x = self.reLU(self.layer5(x))\n",
    "        x = self.layer6(x)\n",
    "        return x\n",
    "\n",
    "model = MLPModel(801, 10000, 5000, 1000, 100, 20, 4).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = optimise.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f069150c-3532-42f8-917b-8897b081c94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch 1/400, Training Loss: 2.467228889465332, Validation Loss: 1.0644907588139176\n",
      "Epoch 2/400, Training Loss: 2.170783042907715, Validation Loss: 0.9560940559022129\n",
      "Epoch 3/400, Training Loss: 2.502584457397461, Validation Loss: 1.0120691158808768\n",
      "Epoch 4/400, Training Loss: 3.1601338386535645, Validation Loss: 1.1094798408448696\n",
      "Epoch 5/400, Training Loss: 3.18863582611084, Validation Loss: 1.0846811840310693\n",
      "Epoch 6/400, Training Loss: 3.634798526763916, Validation Loss: 1.2941246693953872\n",
      "Epoch 7/400, Training Loss: 1.9039463996887207, Validation Loss: 0.8917692806571722\n",
      "Epoch 8/400, Training Loss: 2.747490644454956, Validation Loss: 1.1534282159991562\n",
      "Epoch 9/400, Training Loss: 3.1158368587493896, Validation Loss: 1.2878677789121866\n",
      "Epoch 10/400, Training Loss: 3.319990873336792, Validation Loss: 1.322663526982069\n",
      "Epoch 11/400, Training Loss: 3.3242790699005127, Validation Loss: 1.4002099772915244\n",
      "Epoch 12/400, Training Loss: 3.5290253162384033, Validation Loss: 1.4938866449519992\n",
      "Epoch 13/400, Training Loss: 3.88627028465271, Validation Loss: 1.505122842732817\n",
      "Epoch 14/400, Training Loss: 2.8298380374908447, Validation Loss: 1.2917306572198868\n",
      "Epoch 15/400, Training Loss: 3.5505080223083496, Validation Loss: 1.5017939107492566\n",
      "Epoch 16/400, Training Loss: 2.689875841140747, Validation Loss: 1.4079231908544898\n",
      "Epoch 17/400, Training Loss: 3.3316237926483154, Validation Loss: 1.5144683746621013\n",
      "Epoch 18/400, Training Loss: 4.368583679199219, Validation Loss: 1.731917990371585\n",
      "Epoch 19/400, Training Loss: 3.5345962047576904, Validation Loss: 1.4707601573318243\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[1;32m     26\u001b[0m             loss \u001b[38;5;241m=\u001b[39m criterion(outputs, Y_batch)\n\u001b[0;32m---> 27\u001b[0m             val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(test_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "epochs = 400\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        X_batch, Y_batch = X_batch.to(device, non_blocking=True), Y_batch.to(device, non_blocking=True)\n",
    "        optimiser.zero_grad()\n",
    "        # with torch.cuda.amp.autocast():\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, Y_batch)\n",
    "        # scaler.scale(loss).backward()\n",
    "        # scaler.step(optimiser)\n",
    "        # scaler.update()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, Y_batch)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch in test_loader:\n",
    "            X_batch, Y_batch = X_batch.to(device, non_blocking=True), Y_batch.to(device, non_blocking=True)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, Y_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss / len(test_loader)}\")\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7016d0-1eaa-4872-8bbe-9fe7a530ed38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
